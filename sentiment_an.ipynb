{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: youtube_data/youtube_videos_2024-06-01.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-02.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-03.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-04.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-05.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-06.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-07.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-08.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-09.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-10.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-11.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-12.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-13.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-14.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-15.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-16.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-17.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-18.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-19.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-20.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-21.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-22.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-23.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-24.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-25.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-26.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-27.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-28.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-29.csv\n",
      "File not found: youtube_data/youtube_videos_2024-06-30.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 모든 데이터를 하나의 데이터프레임으로 결합\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/2024-1/stock_ma/stock_ma/lib/python3.9/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/Desktop/2024-1/stock_ma/stock_ma/lib/python3.9/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/Desktop/2024-1/stock_ma/stock_ma/lib/python3.9/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "csv_base_path = 'youtube_data'\n",
    "\n",
    "# 날짜 범위 설정\n",
    "start_date = 1\n",
    "end_date = 30\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 각 날짜에 대해 파일 처리\n",
    "for day in range(start_date, end_date + 1):\n",
    "    date_str = f'2024-06-{day:02d}'\n",
    "    csv_file_path = f'{csv_base_path}/youtube_videos_{date_str}.csv'\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        df['date'] = date_str  # 새로운 열로 날짜 추가\n",
    "        all_data.append(df)\n",
    "        print(f\"Loaded data from {csv_file_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {csv_file_path}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"No data in file: {csv_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {csv_file_path}: {e}\")\n",
    "\n",
    "# 모든 데이터를 하나의 데이터프레임으로 결합\n",
    "df = pd.concat(all_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감정분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiheelee/Desktop/2024-1/stock_ma/stock_ma/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# TensorFlow 기반의 감정 분석 파이프라인 생성\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, framework='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 예시 텍스트\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[43mtitles\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 감정 분석 수행\u001b[39;00m\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m sentiment_pipeline(texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titles' is not defined"
     ]
    }
   ],
   "source": [
    "# 예시 텍스트\n",
    "texts = titles\n",
    "# 감정 분석 수행\n",
    "results = sentiment_pipeline(texts)\n",
    "\n",
    "scores_huggingface = []\n",
    "\n",
    "# 결과 출력\n",
    "for text, result in zip(texts, results):\n",
    "    scores_huggingface.append(result['score'])\n",
    "    print(f\"Text: {text}\\nSentiment: {result['label']}, Score: {result['score']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>This Is How You Can Trade The Nvidia Stock Split</td>\n",
       "      <td>0.998091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>NVDA Nvidia Red or Green on Monday...$1050 or ...</td>\n",
       "      <td>0.992906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>Should I Buy NVDA? | Weekend Edition June 1, 2024</td>\n",
       "      <td>0.995354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>Is NVIDIA Stock A BUY Before Stock Split For M...</td>\n",
       "      <td>0.992762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>NVDA Stock: (NVIDIA stock) NVDA STOCK Predicti...</td>\n",
       "      <td>0.994530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>HAFTAYA BAKIS - NON-FARM PAYROLL HAFTASI - NVD...</td>\n",
       "      <td>0.996137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>$Nvda technical analysis</td>\n",
       "      <td>0.987794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>INSIDERS VENDIENDO en NVDA. Alerta!</td>\n",
       "      <td>0.632810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>ULTY My Next $40,000 Margin ETF?  Full Review ...</td>\n",
       "      <td>0.962295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>META Stock Prediction (FACEBOOK) Best Stock fo...</td>\n",
       "      <td>0.979783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1094 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                              title     score\n",
       "0     2024-06-01   This Is How You Can Trade The Nvidia Stock Split  0.998091\n",
       "1     2024-06-01  NVDA Nvidia Red or Green on Monday...$1050 or ...  0.992906\n",
       "2     2024-06-01  Should I Buy NVDA? | Weekend Edition June 1, 2024  0.995354\n",
       "3     2024-06-01  Is NVIDIA Stock A BUY Before Stock Split For M...  0.992762\n",
       "4     2024-06-01  NVDA Stock: (NVIDIA stock) NVDA STOCK Predicti...  0.994530\n",
       "...          ...                                                ...       ...\n",
       "1089  2024-06-30  HAFTAYA BAKIS - NON-FARM PAYROLL HAFTASI - NVD...  0.996137\n",
       "1090  2024-06-30                           $Nvda technical analysis  0.987794\n",
       "1091  2024-06-30                INSIDERS VENDIENDO en NVDA. Alerta!  0.632810\n",
       "1092  2024-06-30  ULTY My Next $40,000 Margin ETF?  Full Review ...  0.962295\n",
       "1093  2024-06-30  META Stock Prediction (FACEBOOK) Best Stock fo...  0.979783\n",
       "\n",
       "[1094 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ = pd.DataFrame({'date': df['date'], 'title': titles, 'score': scores_huggingface})\n",
    "result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_.to_csv(\"nvidia_june.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mresult_\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistribution of Scores\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(result_['score'], bins=10, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수\n",
    "\n",
    "def sent_analysis(data): \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 튜닝 데이터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_Google",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
